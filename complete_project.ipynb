{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guided project we will be creating our very own book recommender system. To do that we first need to import all the books in book_education folder. It contains 8 books downloaded from\n",
    "[Project Gutenberg - Education](https://www.gutenberg.org/ebooks/bookshelf/138).\n",
    "\n",
    "We will first start by listing all the books in books_education folder. For that we will import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function glob in module glob:\n",
      "\n",
      "glob(pathname, *, recursive=False)\n",
      "    Return a list of paths matching a pathname pattern.\n",
      "    \n",
      "    The pattern may contain simple shell-style wildcards a la\n",
      "    fnmatch. However, unlike fnmatch, filenames starting with a\n",
      "    dot are special cases that are not matched by '*' and '?'\n",
      "    patterns.\n",
      "    \n",
      "    If recursive is true, the pattern '**' will match any files and\n",
      "    zero or more directories and subdirectories.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import glob\n",
    "\n",
    "# Learn what glob does\n",
    "help(glob.glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books_education/ACatechismOfFamiliarThings.txt',\n",
       " 'books_education/theInfantSystem.txt',\n",
       " 'books_education/DrMontessorisOwnHandbook.txt',\n",
       " 'books_education/YouthItsEducationRegimenAndHygiene.txt',\n",
       " 'books_education/aCatalogueOfPlayEquipment.txt',\n",
       " 'books_education/theMindAndItsEducation.txt',\n",
       " 'books_education/theTeachingOfHistory.txt',\n",
       " 'books_education/whatTheSchoolsTeachAndMightTeach.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference to our dataset folder\n",
    "books_path = \"books_education/\"\n",
    "\n",
    "# List all the .txt files and sort them alphabetically\n",
    "files = glob.glob(books_path + '*.txt')\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading books\n",
    "\n",
    "We have path of all books with us, we now want to load the text content in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "\n",
    "books = []\n",
    "titles = []\n",
    "\n",
    "# Loop over files and get contents in books list, \n",
    "# Get books titles like ACatechismOfFamiliarThings in titles list.\n",
    "for filepath in files:\n",
    "    with open(filepath,'r',encoding='utf-8-sig') as f:\n",
    "        books.append(f.read())\n",
    "    titles.append(re.sub('.txt','',os.path.basename(filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of A Catechism of Familiar Things; Their\\nHistory, and the Events Which L'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Do some basic cleaning on books\n",
    "We will now do some primitive cleaning on books by using gensim. We will make it lower cased, tokenize and remove stop words from books. In NLP, first step is usually some basic cleaning such as making the text lowercase, [1 minute podcast - Data Cleaning](https://www.stitcher.com/podcast/anchor-podcasts/code-logic/e/77683140). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'catechism', 'familiar', 'things', 'their', 'history', 'and', 'the', 'events', 'which', 'led', 'their', 'discovery', 'benziger', 'brothers', 'this', 'ebook']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "books = [simple_preprocess(book,min_len=3,max_len=15) for book in books]\n",
    "print(books[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(stopset,txt):\n",
    "    return [word for word in txt if word not in stopset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_words = stopwords.words('english')\n",
    "\n",
    "cleaned_books = [remove_stop(en_words,book) for book in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'catechism', 'familiar', 'things', 'history', 'events', 'led', 'discovery', 'benziger', 'brothers', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_books[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Pick a Book to compare with others.\n",
    "I am going along with theMindAndItsEducation. You can pick any book you want, after that we will proceed for Natural Language Processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_book = 'theMindAndItsEducation'\n",
    "\n",
    "# find book index and assign it to variable my_book_index.\n",
    "for i in range(0,len(titles)):\n",
    "    if(titles[i] == my_book):\n",
    "        my_book_index = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_book_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Stemming or Lemmatization in Natural Language Processing\n",
    "So after you have cleaned the books and tokenized them, there may be words such as create and creation in the books but for us, those two words hold the same meaning. We can use **stemming** which chops the endings such as tion, ing, ly, etc. This process is known as stemming in NLP. Sometimes stemming creates words which does not exist and for that reason we sometimes prefer lemmatization in NLP. For our puropses we will be working with WordNetLemmatizer() from nltk.\n",
    "\n",
    "[Stemming in NLP podcast episode (1 minute)](https://www.stitcher.com/podcast/code-logic/e/77814578?autoplay=true)\n",
    "\n",
    "[Lemmatization in NLP podcast episode (1 minute)]() \n",
    "### Add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_list(lis):\n",
    "    \"\"\"\n",
    "    Used to stem list of words\n",
    "    \n",
    "    :param lis: list of words to stem\n",
    "    :returns: list of stemmed words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    from nltk.stem import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stem_list to all books in tokenized_books.\n",
    "stemmed_books = [stem_list(book) for book in cleaned_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of stemming you can also use lemmatizer.\n",
    "# We will be working with lemmatizer.\n",
    "def lemmatize_list(lis):\n",
    "    \"\"\"\n",
    "    Used to Lemmatize list of words\n",
    "    \n",
    "    :param lis: list of words to lemmatize\n",
    "    :returns: list of lemmatized words\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_books = [lemmatize_list(book) for book in cleaned_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'mind', 'education', 'george', 'herbert', 'betts', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restriction', 'whatsoever', 'may', 'copy', 'give', 'away']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_books[my_book_index][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Creating a Dictionary\n",
    "\n",
    "Now that we have our books cleaned and lemmatized. We want to assign a unique token for every word which in our corpus. BTW, corpus is a collection of doucments and document refers to some text in our case one single book is document and collection of books is corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(lemmatized_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18147"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique words in our corpus (Collection of books)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Bag Of Words\n",
    "\n",
    "In this step we will use our dictionary and create a Bag of Word model. In Short sense, Bag of Word model is a way to represent a document in which we ignore the ordering of words but keep the frequency of word appearence in a document. To learn more about BoW, [check this blog](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method doc2bow in module gensim.corpora.dictionary:\n",
      "\n",
      "doc2bow(document, allow_update=False, return_missing=False) method of gensim.corpora.dictionary.Dictionary instance\n",
      "    Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    document : list of str\n",
      "        Input document.\n",
      "    allow_update : bool, optional\n",
      "        Update self, by adding new tokens from `document` and updating internal corpus statistics.\n",
      "    return_missing : bool, optional\n",
      "        Return missing tokens (tokens present in `document` but not in self) with frequencies?\n",
      "    \n",
      "    Return\n",
      "    ------\n",
      "    list of (int, int)\n",
      "        BoW representation of `document`.\n",
      "    list of (int, int), dict of (str, int)\n",
      "        If `return_missing` is True, return BoW representation of `document` + dictionary with missing\n",
      "        tokens and their frequencies.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.corpora import Dictionary\n",
      "        >>> dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
      "        >>> dct.doc2bow([\"this\", \"is\", \"máma\"])\n",
      "        [(2, 1)]\n",
      "        >>> dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n",
      "        ([(2, 1)], {u'this': 1, u'is': 1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Help on doc2bow method for our dictionary instance.\n",
    "help(dictionary.doc2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = [dictionary.doc2bow(book) for book in lemmatized_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 4), (5, 1), (6, 2), (7, 7), (8, 3), (9, 1), (10, 1), (11, 5), (12, 1), (13, 1), (14, 2), (15, 1), (16, 2), (17, 4), (18, 2), (19, 12)]\n"
     ]
    }
   ],
   "source": [
    "# Inspect bag_of_words. \n",
    "# Also, keep in mind that in (x,y) : \n",
    "# x indicates unique key for word by dictionary\n",
    "# y indicates word's frequency.\n",
    "\n",
    "print(bag_of_words[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'able'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[4] # its x, i.e. in first book able came y times which is 4 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Make our selected book into Pandas Dataframe\n",
    "\n",
    "Remember, my_book_index contains our selected book index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dic_key  freq\n",
      "0        2     1\n",
      "1        3    37\n",
      "2        4    87\n",
      "3        6     1\n",
      "4       10     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "my_book_df = pd.DataFrame(bag_of_words[my_book_index])\n",
    "my_book_df.columns = ['dic_key','freq']\n",
    "print(my_book_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add actual word corressponding to dic_key\n",
    "my_book_df['word'] = [dictionary[ind] for ind in my_book_df['dic_key']]\n",
    "\n",
    "# Sort DataFrame based on freq. (descending)\n",
    "sorted_df = my_book_df.sort_values(by=['freq'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dic_key</th>\n",
       "      <th>freq</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>3728</td>\n",
       "      <td>391</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>3349</td>\n",
       "      <td>335</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>3436</td>\n",
       "      <td>270</td>\n",
       "      <td>mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>2888</td>\n",
       "      <td>262</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>3533</td>\n",
       "      <td>256</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dic_key  freq      word\n",
       "1761     3728   391       one\n",
       "1572     3349   335       may\n",
       "1619     3436   270      mind\n",
       "1373     2888   262  interest\n",
       "1661     3533   256      must"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: TF-IDF\n",
    "\n",
    "As we can see in our previous step, most frequent word does not contain much meaning. If you read [Blog on BOW](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) you would have understood the shortcomings of BOW model which is it gives equal weight to every word in the corpus. We want to give more weight to words which are more frequent in one book but less frequent in other to find the important words or topics around which the book revolves. For that purpose we use Term Frequency-Inverse Document Frequency model.\n",
    "\n",
    "[Reading for TF-IDF](https://en.wikipedia.org/wiki/Tf–idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=8, num_nnz=40316)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "#Create TfidfModel\n",
    "model = TfidfModel(bag_of_words)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get selected book's tfidf by passing it to model\n",
    "my_book_tfidf = model[bag_of_words[my_book_index]]\n",
    "\n",
    "# Convert it to dataframe\n",
    "my_book_tfidf_df = pd.DataFrame(my_book_tfidf)\n",
    "my_book_tfidf_df.columns = ['dic_key','weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add actual word corressponding to dic_key\n",
    "my_book_tfidf_df['word'] = [dictionary[ind] for ind in my_book_tfidf_df['dic_key']]\n",
    "\n",
    "# Sort DataFrame based on freq. (descending)\n",
    "sorted_tfidf_df = my_book_tfidf_df.sort_values(by=['weight'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dic_key</th>\n",
       "      <th>weight</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>10333</td>\n",
       "      <td>0.228991</td>\n",
       "      <td>emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>6462</td>\n",
       "      <td>0.208904</td>\n",
       "      <td>brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>3588</td>\n",
       "      <td>0.176947</td>\n",
       "      <td>nerve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>10420</td>\n",
       "      <td>0.176538</td>\n",
       "      <td>imagination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>2727</td>\n",
       "      <td>0.171578</td>\n",
       "      <td>image</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dic_key    weight         word\n",
       "3427    10333  0.228991      emotion\n",
       "2319     6462  0.208904        brain\n",
       "1281     3588  0.176947        nerve\n",
       "3453    10420  0.176538  imagination\n",
       "992      2727  0.171578        image"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View and see the difference.\n",
    "sorted_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Compute Similarities between book\n",
    "\n",
    "Now we are nearing the end of our book recommendation system. We will first start by computing similarity matrix and see how similar books are to each other. Call help and try to understand about MatrixSimilarity or look at this [example code](https://gist.github.com/clemsos/7692685). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "sim = similarities.MatrixSimilarity(model[bag_of_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09561831, 0.14687465, 0.1254952 , 0.29217362, 0.05132653,\n",
       "       1.        , 0.08905275, 0.05647166], dtype=float32)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sim)[my_book_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YouthItsEducationRegimenAndHygiene'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theMindAndItsEducation'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[my_book_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our similarity matrix to Dataframe.\n",
    "\n",
    "sim_df = pd.DataFrame(list(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change columns.\n",
    "sim_df.columns = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACatechismOfFamiliarThings</th>\n",
       "      <th>theInfantSystem</th>\n",
       "      <th>DrMontessorisOwnHandbook</th>\n",
       "      <th>YouthItsEducationRegimenAndHygiene</th>\n",
       "      <th>aCatalogueOfPlayEquipment</th>\n",
       "      <th>theMindAndItsEducation</th>\n",
       "      <th>theTeachingOfHistory</th>\n",
       "      <th>whatTheSchoolsTeachAndMightTeach</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.166868</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>0.107720</td>\n",
       "      <td>0.042284</td>\n",
       "      <td>0.095618</td>\n",
       "      <td>0.028237</td>\n",
       "      <td>0.019488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166868</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.089581</td>\n",
       "      <td>0.224201</td>\n",
       "      <td>0.044810</td>\n",
       "      <td>0.146875</td>\n",
       "      <td>0.059497</td>\n",
       "      <td>0.036427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072915</td>\n",
       "      <td>0.089581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083411</td>\n",
       "      <td>0.053855</td>\n",
       "      <td>0.125495</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.018163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.107720</td>\n",
       "      <td>0.224201</td>\n",
       "      <td>0.083411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070560</td>\n",
       "      <td>0.292174</td>\n",
       "      <td>0.103167</td>\n",
       "      <td>0.093586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042284</td>\n",
       "      <td>0.044810</td>\n",
       "      <td>0.053855</td>\n",
       "      <td>0.070560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.029071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.095618</td>\n",
       "      <td>0.146875</td>\n",
       "      <td>0.125495</td>\n",
       "      <td>0.292174</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089053</td>\n",
       "      <td>0.056472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.028237</td>\n",
       "      <td>0.059497</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.103167</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.089053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.019488</td>\n",
       "      <td>0.036427</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>0.093586</td>\n",
       "      <td>0.029071</td>\n",
       "      <td>0.056472</td>\n",
       "      <td>0.093358</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACatechismOfFamiliarThings  theInfantSystem  DrMontessorisOwnHandbook  \\\n",
       "0                    1.000001         0.166868                  0.072915   \n",
       "1                    0.166868         1.000001                  0.089581   \n",
       "2                    0.072915         0.089581                  1.000000   \n",
       "3                    0.107720         0.224201                  0.083411   \n",
       "4                    0.042284         0.044810                  0.053855   \n",
       "5                    0.095618         0.146875                  0.125495   \n",
       "6                    0.028237         0.059497                  0.020554   \n",
       "7                    0.019488         0.036427                  0.018163   \n",
       "\n",
       "   YouthItsEducationRegimenAndHygiene  aCatalogueOfPlayEquipment  \\\n",
       "0                            0.107720                   0.042284   \n",
       "1                            0.224201                   0.044810   \n",
       "2                            0.083411                   0.053855   \n",
       "3                            1.000000                   0.070560   \n",
       "4                            0.070560                   1.000000   \n",
       "5                            0.292174                   0.051327   \n",
       "6                            0.103167                   0.014675   \n",
       "7                            0.093586                   0.029071   \n",
       "\n",
       "   theMindAndItsEducation  theTeachingOfHistory  \\\n",
       "0                0.095618              0.028237   \n",
       "1                0.146875              0.059497   \n",
       "2                0.125495              0.020554   \n",
       "3                0.292174              0.103167   \n",
       "4                0.051327              0.014675   \n",
       "5                1.000000              0.089053   \n",
       "6                0.089053              1.000000   \n",
       "7                0.056472              0.093358   \n",
       "\n",
       "   whatTheSchoolsTeachAndMightTeach  \n",
       "0                          0.019488  \n",
       "1                          0.036427  \n",
       "2                          0.018163  \n",
       "3                          0.093586  \n",
       "4                          0.029071  \n",
       "5                          0.056472  \n",
       "6                          0.093358  \n",
       "7                          0.999997  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender(book_title, sim_df):\n",
    "    return titles[list(sim_df[book_title].nlargest(2).index)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YouthItsEducationRegimenAndHygiene'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender(titles[my_book_index],sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theMindAndItsEducation'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender('YouthItsEducationRegimenAndHygiene',sim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Creating Clusters for Better understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "Z = hierarchy.linkage([[i] for i in list(sim)[my_book_index]],'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dn = hierarchy.dendrogram(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda99bcb4b0a0be4110a87e42072b64ae25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
